# Product Context

## Why this project exists

This project aims to develop benchmarking tools for evaluating the abstract reasoning capabilities of models on tasks similar to visual IQ tests, using the ARC dataset.

## Problems it solves

*   Provides a standardized way to evaluate model performance on abstract reasoning tasks.
*   Enables comparison of different models or agent approaches.

## How it should work

A simple agent will process tasks from the ARC dataset, feeding training examples to a model. The model's ability to explain the reasoning behind the input-output transformations will be evaluated.

## User experience goals

*   Easy to configure and run benchmarks.
*   Clear and understandable output of model reasoning.
